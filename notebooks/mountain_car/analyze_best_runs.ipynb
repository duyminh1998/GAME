{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GAME.utils.config import config\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import os\n",
    "import json\n",
    "\n",
    "config_data = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisons, FE, and Stopping Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC3D_GAME_evol_folder_path = os.path.join(config_data['output_path'], '11112022 Evolve MC Maps with GAME 240 FE')\n",
    "trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = []\n",
    "fitness_evaluations = []\n",
    "stopping_generation = []\n",
    "for trial in range(trials):\n",
    "    trial_stats = os.path.join(MC3D_GAME_evol_folder_path, 'MC_trial{}_stats.txt'.format(trial))\n",
    "    trial_df = pd.read_csv(trial_stats, index_col=False, header=0)    \n",
    "    fitness_evaluations.append(trial_df['fitness_evaluations'].max())\n",
    "    comparisons.append(trial_df['comparisons'].sum())\n",
    "    stopping_generation.append(trial_df['generation'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average comparisons: 4639.7, Average FE: 134.9, Average stopping generation: 11.2\n"
     ]
    }
   ],
   "source": [
    "print('Average comparisons: {}, Average FE: {}, Average stopping generation: {}'.format(np.mean(comparisons), np.mean(fitness_evaluations), np.mean(stopping_generation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = []\n",
    "fitness_evaluations = []\n",
    "comparisons_avg = []\n",
    "fitness_eval_avg = []\n",
    "for trial in range(trials):\n",
    "    comparisons = []\n",
    "    fitness_evaluations = []\n",
    "    trial_stats = os.path.join(MC3D_GAME_evol_folder_path, 'MC_trial{}_stats.txt'.format(trial))\n",
    "    trial_df = pd.read_csv(trial_stats, index_col=False, header=0)\n",
    "    best_fitness = max(trial_df[' best_fitness'])\n",
    "    for _, row in trial_df.iterrows():\n",
    "        if row[' best_fitness'] < best_fitness:\n",
    "            fitness_evaluations.append(row[' fitness_evaluations'])\n",
    "            comparisons.append(row[' comparisons'])\n",
    "        else:\n",
    "            break\n",
    "    comparisons_avg.append(sum(comparisons))\n",
    "    fitness_eval_avg.append(fitness_evaluations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150.2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(comparisons_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(fitness_eval_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.90914480720451, 116.09085519279549)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.t.interval(alpha=0.95, df=len(fitness_eval_avg)-1, loc=np.mean(fitness_eval_avg), scale=st.sem(fitness_eval_avg)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAME-RMHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC3D_GAMERHMC_evol_folder_path = os.path.join(config_data['output_path'], '11112022 Evolve MC Maps with GAME_RMHC 240 FE')\n",
    "trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = []\n",
    "fitness_evaluations = []\n",
    "stopping_generation = []\n",
    "for trial in range(trials):\n",
    "    trial_stats = os.path.join(MC3D_GAMERHMC_evol_folder_path, 'trial{}_stats.csv'.format(trial))\n",
    "    trial_df = pd.read_csv(trial_stats, index_col=False, header=0)    \n",
    "    fitness_evaluations.append(trial_df['fitness_evaluations'].max())\n",
    "    comparisons.append(trial_df['comparisons'].sum())\n",
    "    stopping_generation.append(trial_df['generation'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average comparisons: 78.4, Average FE: 39.2, Average stopping generation: 37.2\n"
     ]
    }
   ],
   "source": [
    "print('Average comparisons: {}, Average FE: {}, Average stopping generation: {}'.format(np.mean(comparisons), np.mean(fitness_evaluations), np.mean(stopping_generation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = []\n",
    "fitness_evaluations = []\n",
    "comparisons_avg = []\n",
    "fitness_eval_avg = []\n",
    "for trial in range(trials):\n",
    "    comparisons = []\n",
    "    fitness_evaluations = []\n",
    "    trial_stats = os.path.join(MC3D_GAMERHMC_evol_folder_path, 'MC_trial{}_stats.csv'.format(trial))\n",
    "    trial_df = pd.read_csv(trial_stats, index_col=False, header=0)\n",
    "    best_fitness = max(trial_df['best_fitness'])\n",
    "    for _, row in trial_df.iterrows():\n",
    "        if float(row['best_fitness']) < float(best_fitness):\n",
    "            fitness_evaluations.append(row['fitness_evaluations'])\n",
    "            comparisons.append(row['comparisons'])\n",
    "        elif float(row['best_fitness']) >= float(best_fitness):\n",
    "            break\n",
    "    comparisons_avg.append(sum(comparisons))\n",
    "    fitness_eval_avg.append(fitness_evaluations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(comparisons_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.7"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(fitness_eval_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best mapping and fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC3D_GAME_evol_folder_path = os.path.join(config_data['output_path'], '11142022 Evolve MC Maps with GAME Best Early Stop')\n",
    "trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mappings_fitness = {}\n",
    "final_mappings_count = {}\n",
    "for trial in range(trials):\n",
    "    pop_stats = os.path.join(MC3D_GAME_evol_folder_path, 'trial{}_population_results.txt'.format(trial))\n",
    "    with open(pop_stats, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        found = False\n",
    "        for line in lines:\n",
    "            if found:\n",
    "                split_line = line.split()\n",
    "                mapping_ID = split_line[2][:-1]\n",
    "                mapping_fitness = float(split_line[-1])\n",
    "                if mapping_ID not in final_mappings_count.keys():\n",
    "                    final_mappings_count[mapping_ID] = 1\n",
    "                else:\n",
    "                    final_mappings_count[mapping_ID] = final_mappings_count[mapping_ID] + 1\n",
    "                final_mappings_fitness[mapping_ID] = mapping_fitness\n",
    "            if line.split()[0] == \"Final\":\n",
    "                found = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010100000 0.9901637016516528\n"
     ]
    }
   ],
   "source": [
    "# best mapping and fitness\n",
    "for k, v in final_mappings_fitness.items():\n",
    "    if v == max(final_mappings_fitness.values()):\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010110000 14 0.9888458215199126\n"
     ]
    }
   ],
   "source": [
    "# most frequent mapping and fitness\n",
    "for k, v in final_mappings_count.items():\n",
    "    if v == max(final_mappings_count.values()):\n",
    "        print(k, v, final_mappings_fitness[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAME-RMHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC3D_GAMERHMC_evol_folder_path = os.path.join(config_data['output_path'], '11142022 Evolve MC Maps with GAME_RMHC Best Early Stop')\n",
    "trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mappings_fitness = {}\n",
    "final_mappings_count = {}\n",
    "for trial in range(trials):\n",
    "    pop_stats = os.path.join(MC3D_GAMERHMC_evol_folder_path, 'trial{}_population_results.txt'.format(trial))\n",
    "    with open(pop_stats, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        found = False\n",
    "        for line in lines:\n",
    "            if found:\n",
    "                split_line = line.split()\n",
    "                mapping_ID = split_line[2][:-1]\n",
    "                mapping_fitness = float(split_line[-1])\n",
    "                if mapping_ID not in final_mappings_count.keys():\n",
    "                    final_mappings_count[mapping_ID] = 1\n",
    "                else:\n",
    "                    final_mappings_count[mapping_ID] = final_mappings_count[mapping_ID] + 1\n",
    "                final_mappings_fitness[mapping_ID] = mapping_fitness\n",
    "            if line.split()[0] == \"Final\":\n",
    "                found = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010100000 0.9901637016516528\n"
     ]
    }
   ],
   "source": [
    "# best mapping and fitness\n",
    "for k, v in final_mappings_fitness.items():\n",
    "    if v == max(final_mappings_fitness.values()):\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010100000 2 0.9901637016516528\n",
      "010100001 2 0.9869045835974422\n"
     ]
    }
   ],
   "source": [
    "# most frequent mapping and fitness\n",
    "for k, v in final_mappings_count.items():\n",
    "    if v == max(final_mappings_count.values()):\n",
    "        print(k, v, final_mappings_fitness[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981226820719746"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.9929816541449503, 0.9863922534862493, 0.9827540818142966, 0.9898554363903732, 0.9807461375299111, 0.9727148311728532, 0.992743856876952, 0.9865786993775076, 0.9873256083696892, \n",
    "0.9938926191874691, 0.9863977078527576, 0.9832993967828477, 0.9813449416585194, 0.9650493513874667, 0.9363257347643477])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Mappings Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mappings = []\n",
    "unique_mappings = []\n",
    "duplicate_mappings = []\n",
    "for trial in range(trials):\n",
    "    with open(os.path.join(MC3D_GAME_evol_folder_path, 'MC_trial{}_stats_population_stats.txt'.format(trial)), 'r') as f:\n",
    "        pop_stats = json.load(f)\n",
    "        total_mappings.append(len(pop_stats.keys()))\n",
    "        unique_mappings.append(sum(1 for freq in pop_stats.values() if freq == 1))\n",
    "        duplicate_mappings.append(sum(1 for freq in pop_stats.values() if freq > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total mappings: 28.7, Average unique mappings: 22.6, Average duplicate mappings: 6.1\n"
     ]
    }
   ],
   "source": [
    "print('Average total mappings: {}, Average unique mappings: {}, Average duplicate mappings: {}'.format(np.mean(total_mappings), np.mean(unique_mappings), np.mean(duplicate_mappings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best ending individual and fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAME_SAHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC3D_GAME_evol_folder_path = os.path.join(config_data['output_path'], '11112022 Evolve MC Maps with GAME_SAHC Best Early Stop')\n",
    "trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = []\n",
    "fitness_evaluations = []\n",
    "stopping_generation = []\n",
    "for trial in range(trials):\n",
    "    trial_stats = os.path.join(MC3D_GAME_evol_folder_path, 'MC_trial{}_stats.txt'.format(trial))\n",
    "    trial_df = pd.read_csv(trial_stats, index_col=False, header=0)    \n",
    "    fitness_evaluations.append(trial_df['fitness_evaluations'].max())\n",
    "    comparisons.append(trial_df['comparisons'].sum())\n",
    "    stopping_generation.append(trial_df['generation'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average comparisons: 195.2, Average FE: 97.6, Average stopping generation: 5.9\n"
     ]
    }
   ],
   "source": [
    "print('Average comparisons: {}, Average FE: {}, Average stopping generation: {}'.format(np.mean(comparisons), np.mean(fitness_evaluations), np.mean(stopping_generation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mappings = []\n",
    "unique_mappings = []\n",
    "duplicate_mappings = []\n",
    "for trial in range(trials):\n",
    "    with open(os.path.join(MC3D_GAME_evol_folder_path, 'MC_trial{}_stats_population_stats.txt'.format(trial)), 'r') as f:\n",
    "        pop_stats = json.load(f)\n",
    "        total_mappings.append(len(pop_stats.keys()))\n",
    "        unique_mappings.append(sum(1 for freq in pop_stats.values() if freq == 1))\n",
    "        duplicate_mappings.append(sum(1 for freq in pop_stats.values() if freq > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total mappings: 6.9, Average unique mappings: 6.9, Average duplicate mappings: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Average total mappings: {}, Average unique mappings: {}, Average duplicate mappings: {}'.format(np.mean(total_mappings), np.mean(unique_mappings), np.mean(duplicate_mappings)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66eb0ff22152a42a55105d2627a5529bd8ce21fe81b81dbad1c4e8c0b6b69aac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
